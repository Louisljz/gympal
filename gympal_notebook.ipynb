{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GymPal Exercise Form Detection\n",
    "\n",
    "This notebook contains the consolidated code for the GymPal project, which uses computer vision and pose estimation to detect and analyze exercise form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Process Videos\n",
    "Process raw exercise videos to standardize their length and frame rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_video(input_path, output_path, target_duration=5, target_fps=24):\n",
    "    \"\"\"\n",
    "    Process a video:\n",
    "    1. Crop to target duration (first 5 seconds)\n",
    "    2. Standardize to target FPS\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the video\n",
    "        clip = VideoFileClip(input_path)\n",
    "\n",
    "        # Crop to first 5 seconds\n",
    "        if clip.duration > target_duration:\n",
    "            clip = clip.subclip(0, target_duration)\n",
    "\n",
    "        # Set the FPS\n",
    "        clip = clip.set_fps(target_fps)\n",
    "\n",
    "        # Write the processed video\n",
    "        clip.write_videofile(output_path, codec=\"libx264\", fps=target_fps)\n",
    "\n",
    "        # Close the clip to release resources\n",
    "        clip.close()\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_all_videos(input_dir, output_dir):\n",
    "    \"\"\"Process all videos in the input directory\"\"\"\n",
    "    # Get all video files\n",
    "    video_files = [\n",
    "        f for f in os.listdir(input_dir) if f.endswith((\".mp4\", \".avi\", \".mov\"))\n",
    "    ]\n",
    "\n",
    "    if not video_files:\n",
    "        print(f\"No video files found in {input_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_files)} videos to process\")\n",
    "\n",
    "    # Process each video\n",
    "    for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
    "        input_path = os.path.join(input_dir, video_file)\n",
    "        output_path = os.path.join(output_dir, video_file)\n",
    "        process_video(input_path, output_path)\n",
    "\n",
    "    print(\"Video processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the processing\n",
    "input_dir = \"exercises/deadlift\"\n",
    "output_dir = \"deadlift_processed\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "process_all_videos(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Snapshots\n",
    "Interactive tool to extract and annotate key frames from videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "DISPLAY_WIDTH = 1280\n",
    "DISPLAY_HEIGHT = 720\n",
    "WINDOW_NAME = \"Video Annotation\"\n",
    "\n",
    "SOURCE_DIR = \"deadlift_processed\"\n",
    "SNAPSHOT_DIR = \"deadlift_snapshots\"\n",
    "os.makedirs(SNAPSHOT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def show_start_screen():\n",
    "    \"\"\"Displays a splash screen before annotation starts.\"\"\"\n",
    "    start_screen = np.zeros((DISPLAY_HEIGHT, DISPLAY_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "    cv2.putText(\n",
    "        start_screen,\n",
    "        \"Press any key to start...\",\n",
    "        (int(DISPLAY_WIDTH * 0.3), int(DISPLAY_HEIGHT * 0.5)),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    cv2.imshow(WINDOW_NAME, start_screen)\n",
    "    cv2.waitKey(0)  # Wait indefinitely until a key is pressed\n",
    "\n",
    "\n",
    "def save_snapshot(frame, video_name, frame_pos, phase=None):\n",
    "    \"\"\"Save a snapshot of the current frame with frame number and phase.\"\"\"\n",
    "    # Create video-specific subfolder\n",
    "    video_snapshot_dir = os.path.join(SNAPSHOT_DIR, os.path.splitext(video_name)[0])\n",
    "    os.makedirs(video_snapshot_dir, exist_ok=True)\n",
    "\n",
    "    # Create filename with frame number and phase\n",
    "    phase_str = f\"_{phase}\" if phase else \"\"\n",
    "    filename = f\"frame_{frame_pos}{phase_str}.jpg\"\n",
    "    filepath = os.path.join(video_snapshot_dir, filename)\n",
    "\n",
    "    # Save the frame\n",
    "    cv2.imwrite(filepath, frame)\n",
    "\n",
    "\n",
    "def annotate_video(video_path):\n",
    "    \"\"\"Plays a video for annotation with rewind, fast-forward, and snapshot.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return None\n",
    "\n",
    "    video_name = os.path.basename(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Frames per second\n",
    "    annotations = []\n",
    "    last_key = None\n",
    "    paused = False\n",
    "\n",
    "    while True:\n",
    "        if not paused:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # Stop if the video ends\n",
    "\n",
    "        frame_pos = int(cap.get(cv2.CAP_PROP_POS_FRAMES))  # Get current frame\n",
    "\n",
    "        # Resize and display the frame\n",
    "        frame = cv2.resize(frame, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "        cv2.imshow(WINDOW_NAME, frame)\n",
    "\n",
    "        key = cv2.waitKey(10) & 0xFF  # Adjusted delay for smooth playback\n",
    "\n",
    "        if key == ord(\" \"):  # Pause/Play\n",
    "            paused = not paused\n",
    "\n",
    "        elif key == ord(\"a\"):  # Rewind 1 second\n",
    "            frame_pos = max(0, frame_pos - fps)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos)\n",
    "\n",
    "        elif key == ord(\"d\"):  # Fast-forward 1 second\n",
    "            frame_pos += fps\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos)\n",
    "\n",
    "        elif key == ord(\"t\") and last_key != \"t\":  # Top annotation\n",
    "            annotations.append((frame_pos, \"top\"))\n",
    "            save_snapshot(frame, video_name, frame_pos, \"top\")\n",
    "            last_key = \"t\"\n",
    "\n",
    "        elif key == ord(\"b\") and last_key != \"b\":  # Bottom annotation\n",
    "            annotations.append((frame_pos, \"bottom\"))\n",
    "            save_snapshot(frame, video_name, frame_pos, \"bottom\")\n",
    "            last_key = \"b\"\n",
    "\n",
    "        elif key == ord(\"q\"):  # Quit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all video files\n",
    "video_files = [\n",
    "    os.path.join(SOURCE_DIR, f) for f in os.listdir(SOURCE_DIR) if f.endswith(\".mp4\")\n",
    "]\n",
    "\n",
    "# Store annotations\n",
    "all_annotations = {}\n",
    "\n",
    "def run_annotation():\n",
    "    if video_files:\n",
    "        cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(WINDOW_NAME, DISPLAY_WIDTH, DISPLAY_HEIGHT)\n",
    "\n",
    "        # Show splash screen\n",
    "        show_start_screen()\n",
    "\n",
    "    current_video_index = 0\n",
    "    while current_video_index < len(video_files):\n",
    "        video_file = video_files[current_video_index]\n",
    "\n",
    "        annotations = annotate_video(video_file)\n",
    "        if annotations:\n",
    "            all_annotations[os.path.basename(video_file)] = annotations\n",
    "\n",
    "        # After annotation, ask for next action\n",
    "        action_screen = np.zeros((DISPLAY_HEIGHT, DISPLAY_WIDTH, 3), dtype=np.uint8)\n",
    "        cv2.putText(\n",
    "            action_screen,\n",
    "            f\"Video {current_video_index + 1}/{len(video_files)} completed\",\n",
    "            (int(DISPLAY_WIDTH * 0.3), int(DISPLAY_HEIGHT * 0.4)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            action_screen,\n",
    "            \"Press 'd' for next video, 'a' for previous video, 'q' to quit\",\n",
    "            (int(DISPLAY_WIDTH * 0.2), int(DISPLAY_HEIGHT * 0.5)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.8,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "        cv2.imshow(WINDOW_NAME, action_screen)\n",
    "        key = cv2.waitKey(0) & 0xFF\n",
    "\n",
    "        if key == ord(\"d\"):  # Next video\n",
    "            current_video_index += 1\n",
    "        elif key == ord(\"a\") and current_video_index > 0:  # Previous video\n",
    "            current_video_index -= 1\n",
    "        elif key == ord(\"q\"):  # Quit\n",
    "            break      \n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\nAnnotation complete!\")\n",
    "\n",
    "# Run the annotation code when needed\n",
    "# run_annotation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Keypoints\n",
    "Extract pose keypoints from the annotated frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "SOURCE_DIR = \"deadlift_snapshots\"\n",
    "OUTPUT_DIR = \"deadlift_keypoints\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    min_detection_confidence=0.6,\n",
    ")\n",
    "\n",
    "KEYPOINTS = {\n",
    "    \"shoulder_left\": 11,\n",
    "    \"shoulder_right\": 12,\n",
    "    \"elbow_left\": 13,\n",
    "    \"elbow_right\": 14,\n",
    "    \"hip_left\": 23,\n",
    "    \"hip_right\": 24,\n",
    "    \"knee_left\": 25,\n",
    "    \"knee_right\": 26,\n",
    "    \"ankle_left\": 27,\n",
    "    \"ankle_right\": 28,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints():\n",
    "    csv_filename = \"deadlift_keypoints.csv\"\n",
    "    csv_file = open(csv_filename, \"w\", newline=\"\")\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    header = [\"video_file\", \"frame_no\"]\n",
    "    for keypoint in KEYPOINTS.keys():\n",
    "        header.extend([f\"{keypoint}_x\", f\"{keypoint}_y\", f\"{keypoint}_z\"])\n",
    "    header.append(\"label\")  # Add label column\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    for video_name in tqdm(os.listdir(SOURCE_DIR), desc=\"Detecting keypoints\"):\n",
    "        video_folder = os.path.join(SOURCE_DIR, video_name)\n",
    "\n",
    "        for image_file in os.listdir(video_folder):\n",
    "            parts = image_file.split(\"_\")\n",
    "            frame_no = int(parts[1])\n",
    "            label = parts[2].split('.')[0]\n",
    "            row = [video_name, frame_no]\n",
    "\n",
    "            image_path = os.path.join(video_folder, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "            frame_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            results = pose.process(frame_rgb)\n",
    "            if results.pose_landmarks:\n",
    "                for idx in KEYPOINTS.values():\n",
    "                    landmark = results.pose_landmarks.landmark[idx]\n",
    "                    row.extend([landmark.x, landmark.y, landmark.z])\n",
    "            else:\n",
    "                print(f\"No pose landmarks detected in: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            row.append(label)\n",
    "            csv_writer.writerow(row)\n",
    "    \n",
    "    csv_file.close()\n",
    "    print(f\"Keypoints saved to {csv_filename}\")\n",
    "\n",
    "# Run keypoint extraction when needed\n",
    "# extract_keypoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Augment Training Data\n",
    "Extract additional frames around snapshots to increase training data volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_frames_around_snapshots(source_dir, snapshots_dir, frame_range=2):\n",
    "    \"\"\"\n",
    "    Extract frames around the snapshot frames from processed videos and save them\n",
    "    to the same snapshot directory structure.\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Directory containing processed videos\n",
    "        snapshots_dir: Directory containing snapshots organized by video name\n",
    "        frame_range: Number of frames to extract before and after the snapshot frame\n",
    "    \"\"\"\n",
    "    # Get all video files\n",
    "    video_files = [f for f in os.listdir(source_dir) if f.endswith((\".mp4\", \".avi\", \".mov\"))]\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"No video files found in {source_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(video_files)} videos to process\")\n",
    "    \n",
    "    for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(source_dir, video_file)\n",
    "        \n",
    "        # Check if this video has snapshots\n",
    "        video_snapshot_dir = os.path.join(snapshots_dir, video_name)\n",
    "        if not os.path.exists(video_snapshot_dir):\n",
    "            print(f\"No snapshots found for {video_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Get all snapshot files for this video\n",
    "        snapshot_files = [f for f in os.listdir(video_snapshot_dir) if f.endswith(\".jpg\")]\n",
    "        \n",
    "        if not snapshot_files:\n",
    "            print(f\"No snapshot files found for {video_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Extract frame numbers and phases from snapshot filenames\n",
    "        frame_info = []\n",
    "        for snapshot_file in snapshot_files:\n",
    "            # Example: frame_54_top.jpg\n",
    "            try:\n",
    "                parts = snapshot_file.split('_')\n",
    "                if len(parts) == 3 and parts[0] == \"frame\":\n",
    "                    frame_no = int(parts[1])\n",
    "                    phase = parts[2].split('.')[0]  # Get 'top' or 'bottom' without extension\n",
    "                    frame_info.append((frame_no, phase))\n",
    "            except (ValueError, IndexError):\n",
    "                print(f\"Could not parse frame info from {snapshot_file}, skipping...\")\n",
    "                continue\n",
    "        \n",
    "        # Open the video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Could not open {video_path}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        for frame_no, phase in frame_info:\n",
    "            frames_to_extract = []\n",
    "            for offset in range(-frame_range, frame_range + 1):\n",
    "                # Skip the target frame (offset 0)\n",
    "                if offset == 0:\n",
    "                    continue\n",
    "                \n",
    "                target_frame = frame_no + offset\n",
    "                if 0 <= target_frame < total_frames:\n",
    "                    frames_to_extract.append(target_frame)\n",
    "            \n",
    "            # Extract only the specific frames\n",
    "            for f in frames_to_extract:\n",
    "                # Set the frame position\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, f)\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    print(f\"Could not read frame {f} from {video_path}, skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Save the frame to the same snapshot directory with phase information\n",
    "                frame_filename = f\"frame_{f}_{phase}.jpg\"\n",
    "                output_path = os.path.join(video_snapshot_dir, frame_filename)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "        \n",
    "        # Release the video\n",
    "        cap.release()\n",
    "    \n",
    "    print(\"Frame extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the augmentation\n",
    "source_dir = \"deadlift_processed\"\n",
    "snapshots_dir = \"deadlift_snapshots\"\n",
    "\n",
    "# extract_frames_around_snapshots(source_dir, snapshots_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model\n",
    "Train a machine learning model to classify poses using the extracted keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_model():\n",
    "    # Load dataset\n",
    "    data = pd.read_csv(\"deadlift_keypoints.csv\")\n",
    "\n",
    "    # Drop non-numeric and unnecessary columns\n",
    "    X = data.iloc[:, 2:-1]  # Exclude video_file, frame_no, and label\n",
    "    y = data[\"label\"].map({\"top\": 1, \"bottom\": 0})  # Encode labels\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit StandardScaler only on the training set\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Define models for grid search\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(),\n",
    "        \"SVM\": SVC(),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "    }\n",
    "\n",
    "    # Define parameter grids\n",
    "    param_grids = {\n",
    "        \"RandomForest\": {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20]},\n",
    "        \"SVM\": {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"]},\n",
    "        \"KNN\": {\"n_neighbors\": [3, 5, 7]},\n",
    "    }\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    best_models = {}\n",
    "    for name, model in models.items():\n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grids[name], cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "        print(f\"Best {name} model: {grid_search.best_params_}\")\n",
    "\n",
    "    # Find the best performing model by comparing test accuracy scores\n",
    "    best_model_name = max(best_models, key=lambda k: best_models[k].score(X_test, y_test))\n",
    "    best_model = best_models[best_model_name]\n",
    "    print(f\"Selected best model: {best_model_name}\")\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save the model to disk\n",
    "    model_filename = 'deadlift_classifier_model.pkl'\n",
    "    with open(model_filename, 'wb') as file:\n",
    "        pickle.dump(best_model, file)\n",
    "\n",
    "    scaler_filename = \"deadlift_classifier_scaler.pkl\"\n",
    "    with open(scaler_filename, \"wb\") as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "    \n",
    "    return best_model, scaler\n",
    "\n",
    "# Train the model when needed\n",
    "# best_model, scaler = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference\n",
    "Run real-time inference using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "def run_inference():\n",
    "    class_map = {0: 'bottom', 1: 'top'}\n",
    "    \n",
    "    with open(\"deadlift_classifier_scaler.pkl\", \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    with open(\"deadlift_classifier_model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    mp_utils = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    KEYPOINTS = {\n",
    "        \"shoulder_left\": 11,\n",
    "        \"shoulder_right\": 12,\n",
    "        \"elbow_left\": 13,\n",
    "        \"elbow_right\": 14,\n",
    "        \"hip_left\": 23,\n",
    "        \"hip_right\": 24,\n",
    "        \"knee_left\": 25,\n",
    "        \"knee_right\": 26,\n",
    "        \"ankle_left\": 27,\n",
    "        \"ankle_right\": 28,\n",
    "    }\n",
    "    pose = mp_pose.Pose(min_detection_confidence=0.7, min_tracking_confidence=0.6)\n",
    "\n",
    "    reps = 0\n",
    "    prev_phase = None\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 for webcam, or provide video path\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame)\n",
    "\n",
    "        phase_text = \"-\"\n",
    "        if results.pose_landmarks:\n",
    "            input = []\n",
    "            for idx in KEYPOINTS.values():\n",
    "                landmark = results.pose_landmarks.landmark[idx]\n",
    "                input.extend([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            input = scaler.transform([input])\n",
    "            output = model.predict_proba(input)[0]\n",
    "\n",
    "            idx = np.argmax(output)\n",
    "            prob = output[idx]\n",
    "\n",
    "            if prob > 0.8: # threshold to configure\n",
    "                curr_phase = class_map[idx]\n",
    "                phase_text = curr_phase\n",
    "                # Count a rep when moving from bottom to top\n",
    "                if prev_phase == \"bottom\" and curr_phase == \"top\":\n",
    "                    reps += 1\n",
    "                prev_phase = curr_phase\n",
    "\n",
    "            mp_utils.draw_landmarks(img, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        cv2.putText(img, f\"Phase: {phase_text}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        cv2.putText(img, f\"Reps: {reps}\", (1000, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Deadlift Counter', img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run inference when needed\n",
    "# run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
